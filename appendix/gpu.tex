\zotelo{../thesis.bib}

\chapter{GPU architecture and CUDA}
\label{app:gpu}

% In this appendix we summarize the effective radiative drag due to resonant
% scattering in the magnetosphere of magnetars, as an extension of the physics
% model described in chapter \ref{chap:magnetar}.

\section{The GPU Architecture}
\label{sec:gpu-arch}

In the recent decade we saw the rise of Graphics Processing Units (GPUs) as an
alternative way to carry out general purpose computations. GPUs are originally
designed specifically for intensive floating point calculations that are very
common in video games and 3D rendering. For these applications, typically the
goal is to compute the color of a pixel on the screen, where each pixel is
relatively independent from others. In order to compute millions of pixels every
frame and keep up with more than 60 frames per second, GPUs are designed to be
massive parallel machines. A modern GPU typically has hundreds to thousands of
cores, compared to up to less than 20 cores in a top-of-the line CPU. In terms
of raw floating point operations per second, a single modern GPU can break 5
TetraFLOPs per second which not that many years ago was only achievable by
supercomputers. In addition, the GPU on-board memory has usually 5-6 times more
bandwidth than the ordinary DRAM, which further improves the computation throughput
especially for memory bound codes.

However, the many cores of the modern GPU come at a price, as each is less
competent than a core of an equivalent CPU. The GPU cores are organized into
symmetric multiprocessors (SMPs), and all the cores in an SMP share one control
unit while each core has only an ALU (Arithmetic Logic Unit) which is used to
perform arithmetic and logic operations (figure \ref{fig:gpu-vs-cpu}). This
means that the same statement is carried out not only by one core but by a group
of cores, on a chunk of data at the same time. At the time of this writing, the
popular Nvidia K40 processor is the most common one found in local and national
clusters. It has 15 SMPs each with 192 CUDA cores, giving a total of 2880 cores.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{pics/appendix/gputech_f2.png}
  \caption{Difference in GPU and CPU internal structure.}
  \label{fig:gpu-vs-cpu}
\end{figure}

This particular architecture is perfect for operations that naturally call for
parallelism, but it poses a serious programming challenge for general tasks. For
example, when an algorithm requires iterations where each step depends on the
previous one (e.g. numerical integration of an ODE), there is no way to do it in
parallel, and the GPU structure falls back to executing everything in series,
which is very slow due to the inherently lower clock speed. Another problem lies
deeper in the architecture itself. The many cores in an SMP are organized into
``warps'' of typically 32 cores each. An instruction is passed to a warp and
executed by all 32 cores simultaneously regardless of the content of the
instruction or any conditional statements. If an \verb!if! statement is part of
the instruction and segregates the warp into two branches, then the two branches
will both be executed by all threads, with inappropriate results discarded. The
worst case is when all 32 instances belong to different conditional branches,
the same instructions will be executed 32 times, effectively erasing any
parallelization.

The GPU version of Aperture is developed using the CUDA parallel computing
platform, created by Nvidia Corporation, and designed to be executed on Nvidia
GPUs. The CUDA platform is a software layer that gives direct access to the
GPU's virtual instruction set and parallel computational elements, for the
execution of compute kernels. CUDA is designed to work with programming
languages such as C, C++, and Fortran. Since Aperture was developed in C++, it
is natural to blend CUDA into the code base. In the remaining part of this
appendix, we will describe the execution model of CUDA and outline how I
designed Aperture to take advantage of this model.

\section{The CUDA execution model}
\label{sec:cuda-model}

As mentioned in section \ref{sec:gpu-arch}, GPU relies on massive
parallelization for its speed advantage over ordinary CPUs. This is explicitly
built into the execution model of CUDA. Functions allowed to run on the GPU are
called compute kernels, and are annotated with the keyword \verb!__global__! in
front of the function definition. When a compute kernel is launched, a set of
additional parameters is given specifying the number of threads that will
execute the kernel in parallel. It is very typical to launch hundreds of
thousands of threads at the same time.

Threads in CUDA are organized into thread blocks. Each thread block can have at
most 1024 threads. Multiple blocks can be launched at the same time, and there
is no upper limit on the number of blocks launched. At execution time, thread
blocks are assigned to SMPs. On K40 up to 16 blocks can be sent to an SMP at a
time, and each block is executed in warps of 32 threads. For example, if a
compute kernel is launched with 128 blocks of 512 threads each, in reality up to
64 randomly chosen warps from 16 blocks can be executed at the same time for
each SMP, and it is all up to the scheduler in the SMP to decide at runtime.

Apart from the warp execution model, each thread block also has a small pool
called ``shared memory'' that is accessible by all threads inside the block.
This piece of shared memory is similar to the L1 cache in ordinary
CPUs\footnote{In fact, the user can choose how the 64KB is split between L1
  cache and the shared memory.}, which has very low latency compared to the main
memory. In addition, threads in a given block can write to the shared memory
{\it atomically}, avoiding race condition. The same operation is much more
costly and difficult to do on the main memory. In K40 each block can have up to
64KB of shared memory. If all threads in a block access a common part of the
main memory repeatedly, loading that part of main memory into shared memory
first can accelerate the memory access speed by a factor of more than a hundred.

In general, memory speed is an important aspect of the GPU program design. The
different levels of memory accessible to a program forms a hierarchy in terms of
speed and latency. The system RAM is the slowest, since every access from the
GPU need to go through the PCIe bus, which has a latency of $\sim 1000$ cycles
and a bandwidth of only a fraction of the system RAM bandwidth. The on board GPU
memory (known as {\it global memory} in CUDA) does not suffer from this issue,
and due to design its bandwidth is much higher than the system RAM, however it
still suffers from very high latency: an access to the GPU main memory requires
$\sim 200$ clock cycles. A faster but very limited memory space is the L1
cache/shared memory as mentioned above. An access to the shared memory only has
latency of a few clock cycles, and multiple threads reading the same address can
be done in a single operation. Figure \ref{fig:gpu-memory}% TODO: figure
shows the GPU memory hierarchy and the perspective memory bandwidth/latency.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \draw[fill=blue!10] (0,0) rectangle (10, 6);
    \draw[fill=blue!20] (0,-3) rectangle (10, -1.5);
    \node at (5.0,-2.25) {System RAM};
    \draw[fill=blue!30] (0.5,0.5) rectangle (9.5, 1.5);
    \node at (5,1.0) {Global Memory};
    \draw[->,thick] (2,-1.5) -- (2,0.5);
    \draw[->,thick] (2.5,0.5) -- node[below right] {PCIe Bus, $\sim 10$GB/s} (2.5,-1.5);
    \draw[fill=blue!20] (0.5,2.75) rectangle (4.75, 5.0);
    \draw[fill=blue!30] (0.75,3) rectangle (4.5,4.0);
    \node at (2.75,3.5) {Shared Memory};
    \node at (2.0,4.5) {Thread block};
    \node at (1.0,5.5) {GPU};
    \draw[->,thick] (1.0,1.5) -- (1.0,3.0);
    \draw[->,thick] (1.5,3.0) -- node[right] {$\sim 100$GB/s} (1.5,1.5);
    \begin{scope}[xshift=4.75cm]
      \draw[fill=blue!20] (0.5,2.75) rectangle (4.75, 5.0);
      \draw[fill=blue!30] (0.75,3) rectangle (4.5,4.0);
      \node at (2.75,3.5) {Shared Memory};
      \node at (2.0,4.5) {Thread block};
      \draw[->,thick] (1.0,1.5) -- (1.0,3.0);
      \draw[->,thick] (1.5,3.0) -- (1.5,1.5);
    \end{scope}
  \end{tikzpicture}
  \caption[Memory hierarchy of Nvidia GPUs.]{Memory hierarchy of Nvidia GPUs.
    Shared memory only have a latency of $\sim 1$ clock cycle. Global memory
    (GPU on-board memory) has a latency of $\sim 100$ cycles, and the system RAM
    has an access latency of more than 1000 cycles and relatively low total
    bandwidth.}
  \label{fig:gpu-memory}
\end{figure}

Therefore the optimal strategy is to either keep all the computation data on GPU
main memory, or to overlap memory copy with computation as much as possible.
When processing data on GPU main memory, try to manipulate a local block of data
at a time by loading them to the shared memory of a thread block, do the
calculation in parallel compute kernels, and then save them back to the GPU main
memory.

\section{Parallelization and optimization in Aperture}
\label{sec:optimization}

PIC codes are naturally very suited for GPUs due to the algorithm being readily
parallelizable. A PIC code typically deals with millions to billions of
particles, each relatively independent of each other especially for
collisionless plasma. On a grid scale, the Maxwell solver is also typically
parallel, each cell only requires the information of a few adjacent cells and
updates independently. Moreover, GPU excels at floating point operations which
is the main kind of arithmetic operations used in a PIC code. The only
potential problem for parallelization lies in current deposition, where by
definition multiple particles need to be processed and then write the result to
the same cell, which can only be done serially.

\subsubsection{Particle pusher}
\label{sec:gpu-pusher}

Most of the particle pusher algorithm is completely parallelizable. Given the
values of $E$ and $B$ fields at the particle location, every particle can be
processed independently to update their momenta and positions. This lend itself
well to the parallel structure of GPUs. The only nontrivial optimization is to
interpolate the field values to particle position, which involves frequent random
access to the field array which is not a strength of the GPU architecture.

The way we optimize this part of the code is to subdivide the whole
computational domain in ``tiles'', such that the 6 field components of each tile
can fit into the shared memory of a thread block. In 2D simulations we found
that $8\times 8$ tiles works very nicely. In the compute kernel, a thread block
is assigned to every tile in the domain, and the threads first load the $E$ and
$B$ field values from global memory into the shared memory of the block. Then
the threads in the block work through the particles inside the tile in parallel,
interpolating the field values to the local particle position using the values
in the shared memory, then update the particle momentum using Vay pusher
(section \ref{sec:ptc-pusher}).

% TODO: A graph or pseudo code to demonstrate the process

A small sacrifice for this algorithm is that we need to have the particles
sorted by tile all the time. Therefore at each time step we sort them
immediately after particle move and production of new pairs, since both
operations mess up the particle order: movement between tile boundary, and
because all new particles are added to the end of the particle array. However we
found this to be the best way since current deposition also benefits from having
a sorted particle array (see \ref{sec:gpu-deposit}).

\subsubsection{Current deposition}
\label{sec:gpu-deposit}

Current deposition poses a main problem for a massively parallelized
architecture like the GPU, since by definition many threads (particles) need to
write the same memory location (current values on the grid), which can not be
done in parallel. The only thing we can do is to use duplicates and minimize
clashing memory access as much as possible.

The current deposition in Aperture is handled as follows. Similar to particle
pusher, current deposition delegates the work for each tile to a thread block.
Every thread block maintains a temporary array in the shared memory to store the
deposited current values. Every thread in the block processes one particle at a
time, and uses atomic add to add the deposited result from each particle to the
array. To minimize clashing, each thread $i$ in the block will go through the
$M$ cells in the tile from $i$ mod $M$ to $(i + M)$ mod $M$. On top of this,
since there is some headroom in shared memory, each thread block actually holds
4 different temporary arrays, and particles are evenly split to deposit to the 4
different arrays, only adding up the results in the end.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \fill[yellow!10] (-1,-1) rectangle (0,0);
    \fill[yellow!10] (2,-1) rectangle (4,0);
    \fill[yellow!10] (-1,2) rectangle (0,4);
    \fill[red!10] (4,4) rectangle (5,5);
    \fill[red!10] (0,4) rectangle (2,5);
    \fill[red!10] (4,0) rectangle (5,2);
    \fill[blue!10] (-1,0) rectangle (0,2);
    \fill[blue!10] (-1,4) rectangle (0,5);
    \fill[blue!10] (2,4) rectangle (4,5);
    \fill[green!10] (4,-1) rectangle (5,0);
    \fill[green!10] (0,-1) rectangle (2,0);
    \fill[green!10] (4,2) rectangle (5,4);

    \draw[fill=red!70] (0,0) rectangle (2,2);
    \draw[fill=blue!70] (2,0) rectangle (4,2);
    \draw[fill=green!70] (0,2) rectangle (2,4);
    \draw[fill=yellow!70] (2,2) rectangle (4,4);
    \draw[dashed] (-1.0,0) -- (5.0,0);
    \draw[dashed] (-1.0,2) -- (5.0,2);
    \draw[dashed] (-1.0,4) -- (5.0,4);
    \draw[dashed] (0,-1.0) -- (0,5.0);
    \draw[dashed] (2,-1.0) -- (2,5.0);
    \draw[dashed] (4,-1.0) -- (4,5.0);
  \end{tikzpicture}
  \caption[4-color scheme for current deposition to avoid overlapping of guard
    cells.]{4-color scheme for current deposition to avoid overlapping of guard
    cells. Each square represents a tile, and tiles of the same color are
    processed at the same time. Since no two neighboring tiles are of the same
    color, there is no issue of guard cell overlapping.}
  \label{fig:4-color-scheme}
\end{figure}

Since during one time step, particles might move across tile boundaries,
therefore current deposition needs to be done with a layer of guard cells around
each tile. However this creates a problem where multiple thread blocks may want
to update overlapping regions of the global memory at the same time. The way
around this issue is to color all tiles in the domain in 4 different colors
(figure ): 4 different kernels are invoked in sequence, each only updating the
tiles with one single color. This avoids any memory access issue between thread
blocks and has minimal impact on performance as long as the domain is large
enough, so that each kernel call saturates the compute pipeline.

\subsubsection{Pair creation}
\label{sec:gpu-pair}

Pair creation poses a similar problem as current deposition. Since new particles
are appended to the end of the main particle array, every photon that is to
convert to a pair will need to access the end of the array and race condition
may occur unless the process is serialized.

To avoid race condition, we need to pre-calculate the final position of the
created photon/pair in the particle array. This is done using a two pass scan of
the particle array. During the first pass, the compute kernel examines every
particle in parallel, maintaining a temporary array that marks whether a
particle will emit a photon or a photon will convert to a pair during this
timestep. For every photon emitting particle, the thread will atomically add 1
to a temporary variable marking the total number of photons emitted in this
tile, then store this number in the corresponding position in the marker array,
which will act as the position of the resulting photon in the main photon array.
Then during the second pass, actual photons will be added to the main array in
parallel, using the values in the marker array as offsets.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{scope}[yshift=4cm]
      \draw (0,0) -- (12,0);
      \draw (0,0.5) -- (12,0.5);
      \foreach \i in {0,...,24}
      {
        \draw (0.5*\i, 0) -- (0.5*\i, 0.5);
      }
      \draw[thick] (0,-0.5) -- (0,1.0);
      \draw[thick] (4,-0.5) -- (4,1.0);
      \draw[thick] (8,-0.5) -- (8,1.0);
      \draw[thick] (12,-0.5) -- (12,1.0);
      \foreach \x in {0,3,4,9,11,12,17,19,21,22,23} {
        \node at (0.25 + 0.5 * \x,0.25) {$\checkmark$};
      }
    \end{scope}
    \begin{scope}[yshift=2cm]
      \draw (0,0) -- (12,0);
      \draw (0,0.5) -- (12,0.5);
      \foreach \i in {0,...,24}
      {
        \draw (0.5*\i, 0) -- (0.5*\i, 0.5);
      }
      \draw[thick] (0,-0.5) -- (0,1.0);
      \draw[thick] (4,-0.5) -- (4,1.0);
      \draw[thick] (8,-0.5) -- (8,1.0);
      \draw[thick] (12,-0.5) -- (12,1.0);
      \node at (0.25,0.25) {0};
      \node at (0.25 + 1.5,0.25) {1};
      \node at (0.25 + 2.0,0.25) {2};
      \node at (0.25 + 4.5,0.25) {0};
      \node at (0.25 + 5.5,0.25) {1};
      \node at (0.25 + 6.0,0.25) {2};
      \node at (0.25 + 8.5,0.25) {0};
      \node at (0.25 + 9.5,0.25) {1};
      \node at (0.25 + 10.5,0.25) {2};
      \node at (0.25 + 11.0,0.25) {3};
      \node at (0.25 + 11.5,0.25) {4};
    \end{scope}
    \node at (6, 1.25) {$+3$};
    \node at (10, 1.25) {$+6$};
    \begin{scope}
      \draw (0,0) -- (12,0);
      \draw (0,0.5) -- (12,0.5);
      \foreach \i in {0,...,24}
      {
        \draw (0.5*\i, 0) -- (0.5*\i, 0.5);
      }
      \draw[thick] (0,-0.5) -- (0,1.0);
      \draw[thick] (4,-0.5) -- (4,1.0);
      \draw[thick] (8,-0.5) -- (8,1.0);
      \draw[thick] (12,-0.5) -- (12,1.0);
      \node at (0.25,0.25) {0};
      \node at (0.25 + 1.5,0.25) {1};
      \node at (0.25 + 2.0,0.25) {2};
      \node at (0.25 + 4.5,0.25) {3};
      \node at (0.25 + 5.5,0.25) {4};
      \node at (0.25 + 6.0,0.25) {5};
      \node at (0.25 + 8.5,0.25) {6};
      \node at (0.25 + 9.5,0.25) {7};
      \node at (0.25 + 10.5,0.25) {8};
      \node at (0.25 + 11.0,0.25) {9};
      \node at (0.25 + 11.5,0.25) {10};
    \end{scope}
  \end{tikzpicture}
  \caption[Thematic illustration for parallel pair creation.]{Thematic
    illustration for parallel pair creation. Each square represents a particle
    and the partition represents a tile. During the first pass the compute
    kernel checks for photon-emitting particles, and mark them with the number
    within the tile (first two rows). Then a prefix sum is carried out on the
    number of emitting particles in each tile. During the second pass, the index
    of each emitting particle is added by the number of emitting particles in
    the proceeding tiles, which now represents the absolute position of the
    photon in the final array.}
  \label{fig:2-pass-pair-creation}
\end{figure}


% Local Variables:
% TeX-master: "../thesis"
% zotero-collection: #("16" 0 2 (name "Thesis"))
% End:
